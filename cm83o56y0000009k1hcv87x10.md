---
title: "7 Key Fundamentals AI Concepts Every Developer Needs to Know"
seoTitle: "Essential AI Concepts for Developers"
seoDescription: "Discover the 7 essential AI concepts every developer should master to unlock their creative power and become a leader in innovative AI solutions"
datePublished: Mon Mar 10 2025 23:03:40 GMT+0000 (Coordinated Universal Time)
cuid: cm83o56y0000009k1hcv87x10
slug: 7-key-fundamentals-ai-concepts
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1741647569363/244787bb-7ab1-4162-adeb-2cc405f1422b.png
tags: ai, game-theory, reinforcement-learning, searching-algorithms, heurist, ai-reasoning, markov-decision-process, mdp

---

If you only consume AI tools without understanding their core principles, you‚Äôre just repeating patterns‚Äînot creating new ones. AI is more than just throwing words at the latest version of ChatGPT or any other tool that emerges‚Äîand they will continue to emerge.

At this point, the advances in this field are so enormous that they cannot be stopped. This reminds me of an article I read in the *Harvard Business Review* that stated:

> "The question isn‚Äôt whether AI will be good enough to take on more cognitive tasks but rather how we‚Äôll adapt. Nobel Prize winner Daniel Kahneman is the world‚Äôs leading expert in human judgment and has spent a lifetime documenting its shortcomings. Yes, AI may have flaws, but human reasoning is deeply flawed, too. Therefore, ‚ÄúClearly AI is going to win,‚Äù [Kahneman remarked in 2021](https://www.theguardian.com/books/2021/may/16/daniel-kahneman-clearly-ai-is-going-to-win-how-people-are-going-to-adjust-is-a-fascinating-problem-thinking-fast-and-slow). "How people adjust is a fascinating problem.‚Äù

For me, the true **creative potential of AI** comes when you **understand its core pieces** and rearrange them into something unique‚Äîjust like **an artist mixing new colors**.

This is not just about ‚Äúlearning AI‚Äù, but it‚Äôs about **unlocking your creative power through the fundamentals**‚Äîso you can build, adapt, invent and become a reference in your company.

So let's start!

## Beyond the hype, what is AI?

Before we get into the nuts and bolts, it's essential to define AI beyond buzzwords. It is more than just machine learning models or LLMs predicting the next word in a sentence, but is about building intelligent systems that interact with the world, make decisions, adapt, and solve complex problems.

## Agents: the intelligence behind the systems

If you open social media and look at discussions on AI, you‚Äôll quickly find an overwhelming number of posts about autonomous agents‚Äîagents that control browsers, desktops, and perform tasks autonomously. Or maybe it's just my algorithm that's broken these days.

What bothers me, though, is how this is talked about like it‚Äôs something new‚Äî‚ÄúOh, the next wave of AI is autonomous agents!‚Äù‚Äîwhen in fact, agents have been a fundamental part of AI for decades. AI itself is about agents.

Everything from self-driving cars to game AI, even Alexa playing your favorite song on Spotify, is an agent. And if you don‚Äôt understand agents, you don‚Äôt understand AI.

The definition is simple: An agent is any entity that perceives its environment and acts upon it to achieve goals.

![](https://media.licdn.com/dms/image/v2/D4D12AQHjh0VSifOHvg/article-inline_image-shrink_1000_1488/B4DZVkjSNLHwAQ-/0/1741148727629?e=1747267200&v=beta&t=2PDmgptnTI5ZpUTYsIkYE2QVlxVKKOjyQeHU8d4zv38 align="left")

*Artificial Intelligence: A Modern Approach*

Think about the Pacman enviroment:

![](https://media.licdn.com/dms/image/v2/D4D12AQFkiDruFfX68w/article-inline_image-shrink_1500_2232/B4DZVkh22lGcAY-/0/1741148352921?e=1747267200&v=beta&t=IC-16heKsNwBXtFMWpr9pRHQdntKTwJRPilA9EBYLvU align="left")

*UC Berkeley*

üìñ *Stuart Russell‚Äôs book sums it up well:*

> *In a nutshell, AI has focused on the study and construction of agents that do the right thing. What counts as the right thing is defined by the objective that we provide to the agent.*

For an agent to act **intelligently**, it must **decide what actions to take**.

So, how do agents actually make decisions? **They search.**

## Search: Finding solutions

So, if the next action an agent should take is not obvious, them it need to plan and consider a sequence of actions that form a path to a goal state.

Many AI problems can be reduced to **searching for the best solution** in a vast space of possibilities. It is used in pathfinding, optimization, and problem-solving.

Think about the Pacman environment, in there, a search problem consists of:

\- A state space:

![](https://media.licdn.com/dms/image/v2/D4D12AQEQJ2MSCT8Itw/article-inline_image-shrink_400_744/B4DZVkikp5HAAY-/0/1741148540078?e=1747267200&v=beta&t=e8wNcdkqD3I8R8i-OmXMQIA7jI6QURCIZAwct9nqono align="left")

UC Berkeley

\- A successor function with actions and costs:

![](https://media.licdn.com/dms/image/v2/D4D12AQHP3GmsGW4irg/article-inline_image-shrink_400_744/B4DZVkjDp5HAAY-/0/1741148667054?e=1747267200&v=beta&t=SHtqESJeEOmswLcZPmlM6iWRzxfwGw5EBRO5LKalvj4 align="left")

*UC Berkeley*

\- A start state and a goal state

Search algorithms solves a lot of common problems, think about the NPCs (non-player characters) in video games as agents, they actually use A\* (A-Star) for pathfinding around obstacles in real-time strategy.

And in talking about games, what if there are multiple agents competing? That‚Äôs where Game Theory comes in.

## Game Theory

In few words, game theory studies how rational agents make decisions when outcomes depend not just on one‚Äôs own actions but also on the actions of others.

It might sound surprising, but **many of AI‚Äôs biggest breakthroughs** have come from **teaching computers how to play games**.

Why? Because games provide **structured environments** with:

‚úî **Clear rules**

‚úî **Measurable goals**

‚úî **Immediate feedback**

When agents must make decisions quickly, they rely on **heuristics**‚Äîquick, rule-of-thumb strategies that guide search and decision-making in complex, uncertain environments.

## Heuristics: Guiding the search

When I first started studying AI, I kept hearing the word *heuristic*‚Äîmy professor would ask, ‚ÄúWhat‚Äôs the heuristic for this problem?‚Äù and I‚Äôd have a big question mark on my face.

Eventually, the concept *clicked*: **heuristics are strategies or shortcuts** that help simplify decision-making and reduce the heavy computational load of searching.

Imagine A\*, the algorithm we mentioned for pathfinding. It uses a heuristic‚Äîlike the **Euclidean distance** in a map‚Äîto **guide** its search. Instead of checking every possible path (which could be infinite), the heuristic gives **tips** to our agent about where to look first, making the process *much* faster.

![](https://media.licdn.com/dms/image/v2/D4D12AQFbeUeqzqQ6YA/article-inline_image-shrink_1000_1488/B4DZVkjnYkHAAY-/0/1741148813540?e=1747267200&v=beta&t=E-X5ZElNJqcR_7Vug0wf2IE-672Fn81ASdL5EQ2na_M align="left")

*UC Berkeley*

Most **real-world problems** are way too **complex** to solve optimally all the time, so AI uses heuristics to **approximate** solutions at high speed. If you think of your AI as a detective, heuristics are the *hunches* that lead it toward the best clues without wasting time.

Still, even the best heuristic can‚Äôt control all the **uncertainty** out there‚Äîafter all, real life isn‚Äôt a neat little puzzle like a Pacman board.

## Probabilistic Reasoning

![](https://media.licdn.com/dms/image/v2/D4D12AQHxBCTdxA9G1A/article-inline_image-shrink_1500_2232/B4DZVkj7NtHIAY-/0/1741148895554?e=1747267200&v=beta&t=SVAzxywwjg6cYdpzd9RDEtn0m18zNWAYeA_IqJIn088 align="left")

*UC Berkeley*

In the real world, everything from **partial observability** (your agent doesn‚Äôt see the whole picture) to **random events** to **adversarial behavior** can introduce *loads* of uncertainty. Your agent can‚Äôt always know exactly where it is, what it‚Äôs dealing with, or how the future will unfold.

This is where all those **statistics classes** we took finally pay off. *Yes!*

**Probabilistic reasoning** allows an agent to assess **how likely** different outcomes are and choose actions that **maximize its chances** of achieving a goal.

From **Bayesian Networks** to **Monte Carlo simulations**, probabilistic reasoning is essential for dealing with **messy, real-world scenarios**. In fact, it‚Äôs the backbone of **decision theory**, which states that an agent is rational if it picks the **action that yields the highest expected utility**, averaged over all possible outcomes. This is called the **principle of maximum expected utility (MEU)**.

Translation... combine the power of **probability theory** with **utility theory** (how much you *value* an outcome), and you get **decision theory**‚Äîthe blueprint for making the best **possible** choice in an **uncertain** world.

But how do AI systems plan ahead? That‚Äôs where **Markov Decision Processes (MDP)** come in.

## Markov Decision Processes (MDP)

When our agent needs to **plan ahead** in an uncertain world, it can‚Äôt just rely on a single action sequence. That‚Äôs where **Markov Decision Processes (MDPs)** swoop in as the blueprint for **systematic decision-making** under uncertainty.

What does ‚ÄúMarkov‚Äù mean? it means that **given the current state, the future is independent of the past**. So, each action outcome depends **only** on where we are right now‚Äînot on the entire history of how we got here.

Think about the gridworld below, if the environment were deterministic, a solution would be easy: ***\[Up, Up, Right, Right, Right\]***.

Unfortunately, the environment won‚Äôt always go along with this solution, because the actions are unreliable. *What should we do?*

![](https://media.licdn.com/dms/image/v2/D4D12AQE50Sgkx_qt0Q/article-inline_image-shrink_1500_2232/B4DZVkkQMGHAAU-/0/1741148980688?e=1747267200&v=beta&t=_GUT1C0gMIi0X3LNsJfHi-7IVmhSLv5vdfYmA_UGhbM align="left")

*UC Berkeley*

\- **Transition Model:** We use P(s' | s, a) to define the probability of landing in state s‚Ä≤ if we take action 'a' in state 's'.

\- **Reward Function:** We assign a **reward (or penalty)** for being in certain states or taking certain actions (like ‚àí0.04 per step to encourage faster solutions).

\- **Policy:** Instead of a single path, we define a **policy** œÄ, which tells our agent the **best action to take** for **any state** it finds itself in.

Why does this matter? Because in a chaotic, real-world environment‚Äîwhether it‚Äôs a self-driving car dealing with slippery roads or a robot in a warehouse with uncertain battery life‚Äîthe agent needs a **backup plan** for every possible scenario. That‚Äôs exactly what a **policy** provides.

Keep the above in mind, now let's tie it all together with my favorite topic on this article.

When an agent learns from interacting with an environment defined by an MDP, it typically uses **Reinforcement Learning (RL)**, let's understand it better!

## Reinforcement Learning

That's the gold in the end of the rainbow, when it comes to unlocking sophisticated reasoning capabilities, reinforcement learning (RL) is the master key that ties together all the foundational concepts we've explored‚Äîfrom search and heuristics to probabilistic reasoning and MDPs.

![](https://media.licdn.com/dms/image/v2/D4D12AQEvZDQ8nQzKrg/article-inline_image-shrink_1500_2232/B4DZVkkaDSGcAY-/0/1741149021698?e=1747267200&v=beta&t=dLSz8Fc-pdYaJ7Rzn0S-ylBNAMVdFhhKxyUSlL1SRag align="left")

*UC Berkeley*

As described in Stuart Russell‚Äôs work, reinforcement learning allows agents to interact with their environment, learning through rewards and penalties rather than relying solely on pre-labeled examples.

In contrast to supervised learning, where millions of examples may be needed, RL leverages simple reward signals (like win/loss or progress metrics) to guide learning, and in a context where training a LLM is very costly, it can be the key to reduce it.

**The DeepSeek papers showcase this beautifully.** DeepSeek-R1-Zero, trained with pure RL, naturally developed impressive reasoning behaviors‚Äîself-verification, extended chain-of-thought, and even spontaneous ‚Äúaha moments.‚Äù

## Experiment, Experiment, Experiment!

Just **reading about AI won‚Äôt make you good at it**. The best way to **unlock creativity and AI expertise** is to **play with AI in experimental settings**.

üîπ **Gymnasium (OpenAI‚Äôs Gym fork)**: A **sandbox for RL**, train AI to play games, control robots, or optimize tasks.

üîπ **Pacman AI (Berkeley Course)**: A **hands-on project** where you build AI agents for Pacman‚Äî perfect for learning Artificial Intelligence with a practical project

AI is **not just about models‚Äîit‚Äôs about experimentation**. The more you **test, break, and rebuild AI**, the more you‚Äôll **innovate** and **become a reference in your field**.

## Conclusion

If you want to go **beyond being a user of AI and become a builder**, start by:

‚úÖ **Understand the AI fundamentals**: like search, decision-making, probabilistic reasoning, RL

‚úÖ **Study Reinforcement Learning & Decision Theory**

‚úÖ **Experimenting with OpenAI Gymnasium, Pacman AI, and multi-agent AI simulations**

**Follow me on** [**linkedin**](https://www.linkedin.com/in/oliveiasmim/) **and keep an eye on my blog (creativedev.art)** if you want **more hands-on insights** on building creative AI solutions.

See you on the next article! üöÄ

## Reference:

\- Stuart Russell & Peter Norvig‚Äôs Artificial Intelligence: A Modern Approach

\- UC Berkeley CS188 Intro to AI

\- AlphaGo Documentary from Google DeepMind

\- DeepSeek Research Papers